{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d300108-14b2-4fa0-bce2-3613a3cd80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "#os.environ['OMP_NUM_THREADS'] = '1'\n",
    "from sklearn.decomposition import PCA, FastICA, NMF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.linalg import eigh\n",
    "from numpy.linalg import pinv, norm\n",
    "from scipy.stats import kurtosis\n",
    "from itertools import combinations\n",
    "import time\n",
    "from sklearn.impute import SimpleImputer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import kaleido\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5f1d80-3cb0-4759-8890-d8e7f254bb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations PCA,ICA,NMF - each has own different number of components automatically found by using reconst. error (frobenius)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create output directory for plots if it doesn't exist\n",
    "output_dir = \"output_plots\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load datasets into a dictionary (these variables should be defined elsewhere)\n",
    "datasets = {\n",
    "    \"Combined Autism\": autismcombinedcommon,\n",
    "    \"Twitch\": twitch,\n",
    "    \"Accent\": accent,\n",
    "    \"Dim512\": dim512,\n",
    "    \"Dim1024\": dim1024\n",
    "}\n",
    "\n",
    "# Store dimensionality reduction results\n",
    "results = []\n",
    "\n",
    "def find_extreme_loadings(components, feature_names, method_name, dataset_name):\n",
    "    \"\"\"\n",
    "    For the top 5 components (or fewer if not available), identify and print the 7 most extreme loadings\n",
    "    by absolute value per component.\n",
    "    \"\"\"\n",
    "    n_components = min(components.shape[0], 5)\n",
    "    for i in range(n_components):\n",
    "        comp = components[i, :]\n",
    "        top_indices = np.argsort(np.abs(comp))[-7:][::-1]\n",
    "        print(f\"\\nTop 7 Extreme Loadings for {dataset_name} ({method_name}) - Component {i+1}:\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"{feature_names[idx]}: {comp[idx]:.4f}\")\n",
    "\n",
    "def apply_dim_reduction(dataset_name, df):\n",
    "    print(f\"\\n--- Processing {dataset_name} Dataset ---\")\n",
    "    df_original = df.copy()\n",
    "    # Exclude the group/dummy columns (id columns have been manually removed)\n",
    "    exclude_columns = [col for col in df.columns if col.startswith(\"language_\") or col.startswith(\"Group_\")]\n",
    "    color_data = df_original[exclude_columns]\n",
    "    X = df.drop(columns=exclude_columns).values\n",
    "    feature_names = df.drop(columns=exclude_columns).columns\n",
    "\n",
    "    # ----------------------- PCA -----------------------\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(0.95, random_state=42)\n",
    "    pca_result = pca.fit_transform(X_scaled)\n",
    "    # Reconstruct the data using inverse transform\n",
    "    reconstructed = pca.inverse_transform(pca_result)\n",
    "    # Absolute reconstruction error (Frobenius norm)\n",
    "    pca_reconstruction_error = np.linalg.norm(X_scaled - reconstructed, 'fro')\n",
    "    # Relative error = error divided by norm of scaled input\n",
    "    pca_relative_error = pca_reconstruction_error / np.linalg.norm(X_scaled, 'fro')\n",
    "    pca_components = pca.n_components_\n",
    "    find_extreme_loadings(pca.components_, feature_names, \"PCA\", dataset_name)\n",
    "\n",
    "    # ----------------------- ICA -----------------------\n",
    "    best_ica_components, best_ica_result, best_ica_error, best_ica_mixing = 1, None, np.inf, None\n",
    "    for n_components in range(1, X_scaled.shape[1] // 2):\n",
    "        ica = FastICA(n_components=n_components, random_state=42, max_iter=30000)\n",
    "        try:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "                ica_result = ica.fit_transform(X_scaled)\n",
    "                # Fixed reconstruction using the transpose of the mixing matrix\n",
    "                reconstructed = ica_result @ ica.mixing_.T\n",
    "                ica_error = np.linalg.norm(X_scaled - reconstructed, 'fro')\n",
    "                if any(issubclass(warning.category, ConvergenceWarning) for warning in w):\n",
    "                    print(f\"[ICA Warning] n_components={n_components}, n_iter_={ica.n_iter_}, error={ica_error:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ICA failed for n_components={n_components}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if ica_error < best_ica_error:\n",
    "            best_ica_error = ica_error\n",
    "            best_ica_components = n_components\n",
    "            best_ica_result = ica_result\n",
    "            best_ica_mixing = ica.mixing_\n",
    "    if best_ica_mixing is not None:\n",
    "        find_extreme_loadings(best_ica_mixing.T, feature_names, \"ICA\", dataset_name)\n",
    "    # Relative error for ICA computed on the same scaled data:\n",
    "    best_ica_relative_error = best_ica_error / np.linalg.norm(X_scaled, 'fro')\n",
    "\n",
    "    # ----------------------- NMF -----------------------\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    X_non_negative = minmax_scaler.fit_transform(X)\n",
    "\n",
    "    best_nmf_components = 1\n",
    "    best_nmf_result = None\n",
    "    best_nmf_error = np.inf\n",
    "    best_nmf_components_matrix = None\n",
    "\n",
    "    for n_components in range(1, X_scaled.shape[1] // 2):\n",
    "        nmf = NMF(n_components=n_components, max_iter=30000, tol=1e-3, random_state=42, init='nndsvda')\n",
    "        try:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "                nmf_result = nmf.fit_transform(X_non_negative)\n",
    "                reconstructed = np.dot(nmf_result, nmf.components_)\n",
    "                nmf_error = np.linalg.norm(X_non_negative - reconstructed, 'fro')\n",
    "                if any(issubclass(warning.category, ConvergenceWarning) for warning in w):\n",
    "                    print(f\"[NMF Warning] n_components={n_components}, n_iter_={nmf.n_iter_}, error={nmf_error:.4f} (tol={nmf.tol})\")\n",
    "        except Exception as e:\n",
    "            print(f\"NMF failed for n_components = {n_components}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if nmf_error < best_nmf_error:\n",
    "            best_nmf_error = nmf_error\n",
    "            best_nmf_components = n_components\n",
    "            best_nmf_result = nmf_result\n",
    "            best_nmf_components_matrix = nmf.components_\n",
    "    find_extreme_loadings(best_nmf_components_matrix, feature_names, \"NMF\", dataset_name)\n",
    "    # Relative error for NMF computed on nonnegative-scaled data:\n",
    "    best_nmf_relative_error = best_nmf_error / np.linalg.norm(X_non_negative, 'fro')\n",
    "\n",
    "    results.append({\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"PCA Components\": pca_components,\n",
    "        \"PCA Reconstruction Error (abs)\": round(pca_reconstruction_error, 4),\n",
    "        \"PCA Relative Error\": round(pca_relative_error, 4),\n",
    "        \"ICA Components\": best_ica_components,\n",
    "        \"ICA Reconstruction Error (abs)\": round(best_ica_error, 4),\n",
    "        \"ICA Relative Error\": round(best_ica_relative_error, 4),\n",
    "        \"NMF Components\": best_nmf_components,\n",
    "        \"NMF Reconstruction Error (abs)\": round(best_nmf_error, 4),\n",
    "        \"NMF Relative Error\": round(best_nmf_relative_error, 4)\n",
    "    })\n",
    "\n",
    "    visualize_dim_reduction(dataset_name, pca, pca_result, best_ica_result, best_nmf_result, \n",
    "                              pca.components_, color_data, best_nmf_components, best_ica_mixing,\n",
    "                              best_nmf_components_matrix)\n",
    "\n",
    "def visualize_dim_reduction(dataset_name, pca, pca_result, ica_result, nmf_result, \n",
    "                            pca_components_matrix, color_data, nmf_components, ica_mixing,\n",
    "                            nmf_components_matrix):\n",
    "    methods = {\"PCA\": pca_result, \"ICA\": ica_result, \"NMF\": nmf_result}\n",
    "\n",
    "    # Combine dummy columns into a single group column.\n",
    "    group_labels = None\n",
    "    custom_colors = None\n",
    "    if \"Accent\" in dataset_name:\n",
    "        group_cols = [col for col in color_data.columns if col.startswith(\"language_\")]\n",
    "        if group_cols:\n",
    "            def get_group(row):\n",
    "                if row[group_cols].sum() == 0:\n",
    "                    return \"language_ES\"\n",
    "                else:\n",
    "                    for col in group_cols:\n",
    "                        if row[col] == 1:\n",
    "                            return col\n",
    "            color_data = color_data.copy()\n",
    "            color_data['group'] = color_data.apply(get_group, axis=1)\n",
    "            group_labels = color_data['group']\n",
    "            custom_colors = px.colors.qualitative.Plotly[:6]  # 6 colors for Accent dataset\n",
    "    elif \"Autism\" in dataset_name:\n",
    "        group_cols = [col for col in color_data.columns if col.startswith(\"Group_\")]\n",
    "        if group_cols:\n",
    "            def get_group(row):\n",
    "                if row[group_cols].sum() == 0:\n",
    "                    return \"Group_Adolescent\"\n",
    "                else:\n",
    "                    for col in group_cols:\n",
    "                        if row[col] == 1:\n",
    "                            return col\n",
    "            color_data = color_data.copy()\n",
    "            color_data['group'] = color_data.apply(get_group, axis=1)\n",
    "            group_labels = color_data['group']\n",
    "            custom_colors = px.colors.qualitative.Plotly[:3]  # 3 colors for Combined Autism dataset\n",
    "\n",
    "    for method_name, result in methods.items():\n",
    "        print(f\"\\nVisualizing {method_name} results for {dataset_name}...\")\n",
    "\n",
    "        # 3D scatter plot using Plotly Express\n",
    "        reduced_df = pd.DataFrame(result[:, :3], columns=[f'{method_name} 1', f'{method_name} 2', f'{method_name} 3'])\n",
    "        if group_labels is not None:\n",
    "            reduced_df['group'] = group_labels.values\n",
    "            fig = px.scatter_3d(\n",
    "                reduced_df, \n",
    "                x=f'{method_name} 1', \n",
    "                y=f'{method_name} 2', \n",
    "                z=f'{method_name} 3',\n",
    "                color='group',\n",
    "                title=f'3D Scatter Plot for {dataset_name} ({method_name})',\n",
    "                color_discrete_sequence=custom_colors\n",
    "            )\n",
    "        else:\n",
    "            fig = px.scatter_3d(\n",
    "                reduced_df, \n",
    "                x=f'{method_name} 1', \n",
    "                y=f'{method_name} 2', \n",
    "                z=f'{method_name} 3',\n",
    "                title=f'3D Scatter Plot for {dataset_name} ({method_name})'\n",
    "            )\n",
    "        plotly_filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_3D.png\")\n",
    "        fig.write_image(plotly_filename)\n",
    "        fig.show()\n",
    "\n",
    "        # Heatmaps for feature loadings\n",
    "        if method_name == \"PCA\":\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(pca_components_matrix[:5], cmap='coolwarm')\n",
    "            plt.title(f'{dataset_name} ({method_name}) Feature Loadings')\n",
    "            filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_heatmap.png\")\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        if method_name == \"ICA\" and ica_mixing is not None:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(ica_mixing.T[:5, :], cmap='coolwarm')\n",
    "            plt.title(f'{dataset_name} ({method_name}) Feature Loadings')\n",
    "            filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_heatmap.png\")\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        if method_name == \"NMF\":\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(nmf_components_matrix[:5, :], cmap='coolwarm')\n",
    "            plt.title(f'{dataset_name} ({method_name}) Feature Loadings')\n",
    "            filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_heatmap.png\")\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "        # --------------------- 2D Pairwise Scatter Plots ---------------------\n",
    "        n_components = min(result.shape[1], 5)\n",
    "        pair_df = pd.DataFrame(result[:, :n_components], columns=[f'{method_name} {i+1}' for i in range(n_components)])\n",
    "        if group_labels is not None:\n",
    "            pair_df['group'] = group_labels.values\n",
    "            pairplot = sns.pairplot(pair_df, hue='group', diag_kind=\"kde\", palette=custom_colors)\n",
    "        else:\n",
    "            pairplot = sns.pairplot(pair_df, diag_kind=\"kde\")\n",
    "        plt.suptitle(f'Pairwise 2D Components for {dataset_name} ({method_name})', y=1.02)\n",
    "        filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_pairplot.png\")\n",
    "        pairplot.fig.savefig(filename, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# Process each dataset\n",
    "for dataset_name, df in datasets.items():\n",
    "    apply_dim_reduction(dataset_name, df)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Dimensionality Reduction Summary ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Save the summary results to a CSV file\n",
    "results_df.to_csv(\"dimensionality_reduction_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09708474-833c-47cf-886b-4f99754e5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations PCA,ICA,NMF - each has own different number of components automatically found by using L1 error (not frobenius)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create output directory for plots if it doesn't exist\n",
    "output_dir = \"output_plots\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Load datasets into a dictionary (these variables should be defined elsewhere)\n",
    "datasets = {\n",
    "    \"Combined Autism\": autismcombinedcommon,\n",
    "    \"Twitch\": twitch,\n",
    "    \"Accent\": accent,\n",
    "    \"Dim512\": dim512,\n",
    "    \"Dim1024\": dim1024\n",
    "}\n",
    "\n",
    "# Store dimensionality reduction results\n",
    "results = []\n",
    "\n",
    "def find_extreme_loadings(components, feature_names, method_name, dataset_name):\n",
    "    \"\"\"\n",
    "    For the top 5 components (or fewer if not available), identify and print the 7 most extreme loadings\n",
    "    by absolute value per component.\n",
    "    \"\"\"\n",
    "    n_components = min(components.shape[0], 5)\n",
    "    for i in range(n_components):\n",
    "        comp = components[i, :]\n",
    "        top_indices = np.argsort(np.abs(comp))[-7:][::-1]\n",
    "        print(f\"\\nTop 7 Extreme Loadings for {dataset_name} ({method_name}) - Component {i+1}:\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"{feature_names[idx]}: {comp[idx]:.4f}\")\n",
    "\n",
    "def apply_dim_reduction(dataset_name, df):\n",
    "    print(f\"\\n--- Processing {dataset_name} Dataset ---\")\n",
    "    df_original = df.copy()\n",
    "    # Exclude the group/dummy columns (id columns have been manually removed)\n",
    "    exclude_columns = [col for col in df.columns if col.startswith(\"language_\") or col.startswith(\"Group_\")]\n",
    "    color_data = df_original[exclude_columns]\n",
    "    X = df.drop(columns=exclude_columns).values\n",
    "    feature_names = df.drop(columns=exclude_columns).columns\n",
    "\n",
    "    # ----------------------- PCA -----------------------\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    pca = PCA(0.95, random_state=42)\n",
    "    pca_result = pca.fit_transform(X_scaled)\n",
    "    # Reconstruct the data using inverse transform\n",
    "    reconstructed = pca.inverse_transform(pca_result)\n",
    "    # Calculate L1 reconstruction error (sum of absolute differences)\n",
    "    pca_reconstruction_error = np.sum(np.abs(X_scaled - reconstructed))\n",
    "    # Relative error for PCA is based on the scaled data\n",
    "    pca_relative_error = pca_reconstruction_error / np.sum(np.abs(X_scaled))\n",
    "    pca_components = pca.n_components_\n",
    "    find_extreme_loadings(pca.components_, feature_names, \"PCA\", dataset_name)\n",
    "\n",
    "    # ----------------------- ICA -----------------------\n",
    "    best_ica_components, best_ica_result, best_ica_error, best_ica_mixing = 1, None, np.inf, None\n",
    "    for n_components in range(1, X_scaled.shape[1] // 2):\n",
    "        ica = FastICA(n_components=n_components, random_state=42, max_iter=30000)\n",
    "        try:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "                ica_result = ica.fit_transform(X_scaled)\n",
    "                # Fixed reconstruction using the transpose of the mixing matrix\n",
    "                reconstructed = ica_result @ ica.mixing_.T\n",
    "                # Calculate L1 reconstruction error\n",
    "                ica_error = np.sum(np.abs(X_scaled - reconstructed))\n",
    "                if any(issubclass(warning.category, ConvergenceWarning) for warning in w):\n",
    "                    print(f\"[ICA Warning] n_components={n_components}, n_iter_={ica.n_iter_}, error={ica_error:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"ICA failed for n_components={n_components}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        if ica_error < best_ica_error:\n",
    "            best_ica_error = ica_error\n",
    "            best_ica_components = n_components\n",
    "            best_ica_result = ica_result\n",
    "            best_ica_mixing = ica.mixing_\n",
    "    if best_ica_mixing is not None:\n",
    "        find_extreme_loadings(best_ica_mixing.T, feature_names, \"ICA\", dataset_name)\n",
    "    # Relative error for ICA on the scaled data\n",
    "    best_ica_relative_error = best_ica_error / np.sum(np.abs(X_scaled))\n",
    "\n",
    "    # ----------------------- NMF -----------------------\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    X_non_negative = minmax_scaler.fit_transform(X)\n",
    "\n",
    "    best_nmf_components = 1\n",
    "    best_nmf_result = None\n",
    "    best_nmf_error = np.inf\n",
    "    best_nmf_components_matrix = None\n",
    "\n",
    "    for n_components in range(1, X_scaled.shape[1] // 2):\n",
    "        nmf = NMF(n_components=n_components, max_iter=30000, tol=1e-3, random_state=42, init='nndsvda')\n",
    "        try:\n",
    "            with warnings.catch_warnings(record=True) as w:\n",
    "                warnings.simplefilter(\"always\", ConvergenceWarning)\n",
    "                nmf_result = nmf.fit_transform(X_non_negative)\n",
    "                reconstructed = np.dot(nmf_result, nmf.components_)\n",
    "                # Calculate L1 reconstruction error for NMF (on nonnegative data)\n",
    "                nmf_error = np.sum(np.abs(X_non_negative - reconstructed))\n",
    "                if any(issubclass(warning.category, ConvergenceWarning) for warning in w):\n",
    "                    print(f\"[NMF Warning] n_components={n_components}, n_iter_={nmf.n_iter_}, error={nmf_error:.4f} (tol={nmf.tol})\")\n",
    "        except Exception as e:\n",
    "            print(f\"NMF failed for n_components = {n_components}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if nmf_error < best_nmf_error:\n",
    "            best_nmf_error = nmf_error\n",
    "            best_nmf_components = n_components\n",
    "            best_nmf_result = nmf_result\n",
    "            best_nmf_components_matrix = nmf.components_\n",
    "    find_extreme_loadings(best_nmf_components_matrix, feature_names, \"NMF\", dataset_name)\n",
    "    # Relative error for NMF based on the nonnegative data:\n",
    "    best_nmf_relative_error = best_nmf_error / np.sum(np.abs(X_non_negative))\n",
    "\n",
    "    results.append({\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"PCA Components\": pca_components,\n",
    "        \"PCA Reconstruction Error\": round(pca_reconstruction_error, 4),\n",
    "        \"PCA Relative Error\": round(pca_relative_error, 4),\n",
    "        \"ICA Components\": best_ica_components,\n",
    "        \"ICA Reconstruction Error\": round(best_ica_error, 4),\n",
    "        \"ICA Relative Error\": round(best_ica_relative_error, 4),\n",
    "        \"NMF Components\": best_nmf_components,\n",
    "        \"NMF Reconstruction Error\": round(best_nmf_error, 4),\n",
    "        \"NMF Relative Error\": round(best_nmf_relative_error, 4)\n",
    "    })\n",
    "    \n",
    "    visualize_dim_reduction(dataset_name, pca, pca_result, best_ica_result, best_nmf_result, \n",
    "                              pca.components_, color_data, best_nmf_components, best_ica_mixing,\n",
    "                              best_nmf_components_matrix)\n",
    "\n",
    "def visualize_dim_reduction(dataset_name, pca, pca_result, ica_result, nmf_result, \n",
    "                            pca_components_matrix, color_data, nmf_components, ica_mixing,\n",
    "                            nmf_components_matrix):\n",
    "    methods = {\"PCA\": pca_result, \"ICA\": ica_result, \"NMF\": nmf_result}\n",
    "\n",
    "    # Combine dummy columns into a single group column.\n",
    "    group_labels = None\n",
    "    custom_colors = None\n",
    "    if \"Accent\" in dataset_name:\n",
    "        group_cols = [col for col in color_data.columns if col.startswith(\"language_\")]\n",
    "        if group_cols:\n",
    "            def get_group(row):\n",
    "                if row[group_cols].sum() == 0:\n",
    "                    return \"language_ES\"\n",
    "                else:\n",
    "                    for col in group_cols:\n",
    "                        if row[col] == 1:\n",
    "                            return col\n",
    "            color_data = color_data.copy()\n",
    "            color_data['group'] = color_data.apply(get_group, axis=1)\n",
    "            group_labels = color_data['group']\n",
    "            custom_colors = px.colors.qualitative.Plotly[:6]  # 6 colors for Accent dataset\n",
    "    elif \"Autism\" in dataset_name:\n",
    "        group_cols = [col for col in color_data.columns if col.startswith(\"Group_\")]\n",
    "        if group_cols:\n",
    "            def get_group(row):\n",
    "                if row[group_cols].sum() == 0:\n",
    "                    return \"Group_Adolescent\"\n",
    "                else:\n",
    "                    for col in group_cols:\n",
    "                        if row[col] == 1:\n",
    "                            return col\n",
    "            color_data = color_data.copy()\n",
    "            color_data['group'] = color_data.apply(get_group, axis=1)\n",
    "            group_labels = color_data['group']\n",
    "            custom_colors = px.colors.qualitative.Plotly[:3]  # 3 colors for Combined Autism dataset\n",
    "\n",
    "    for method_name, result in methods.items():\n",
    "        print(f\"\\nVisualizing {method_name} results for {dataset_name}...\")\n",
    "\n",
    "        # 3D scatter plot using Plotly Express\n",
    "        # If the result has fewer than 3 dimensions, pad with zeros.\n",
    "        if result.shape[1] < 3:\n",
    "            pad_width = 3 - result.shape[1]\n",
    "            padded = np.hstack([result, np.zeros((result.shape[0], pad_width))])\n",
    "            reduced_df = pd.DataFrame(padded, columns=[f'{method_name} 1', f'{method_name} 2', f'{method_name} 3'])\n",
    "        else:\n",
    "            reduced_df = pd.DataFrame(result[:, :3], columns=[f'{method_name} 1', f'{method_name} 2', f'{method_name} 3'])\n",
    "        if group_labels is not None:\n",
    "            reduced_df['group'] = group_labels.values\n",
    "            fig = px.scatter_3d(\n",
    "                reduced_df, \n",
    "                x=f'{method_name} 1', \n",
    "                y=f'{method_name} 2', \n",
    "                z=f'{method_name} 3',\n",
    "                color='group',\n",
    "                title=f'3D Scatter Plot for {dataset_name} ({method_name})',\n",
    "                color_discrete_sequence=custom_colors\n",
    "            )\n",
    "        else:\n",
    "            fig = px.scatter_3d(\n",
    "                reduced_df, \n",
    "                x=f'{method_name} 1', \n",
    "                y=f'{method_name} 2', \n",
    "                z=f'{method_name} 3',\n",
    "                title=f'3D Scatter Plot for {dataset_name} ({method_name})'\n",
    "            )\n",
    "        plotly_filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_3D_L1.png\")\n",
    "        fig.write_image(plotly_filename)\n",
    "        fig.show()\n",
    "\n",
    "        # Heatmaps for feature loadings\n",
    "        if method_name == \"PCA\":\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(pca_components_matrix[:5], cmap='coolwarm')\n",
    "            plt.title(f'{dataset_name} ({method_name}) Feature Loadings')\n",
    "            filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_heatmap_L1.png\")\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        if method_name == \"ICA\" and ica_mixing is not None:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.heatmap(ica_mixing.T[:5, :], cmap='coolwarm')\n",
    "            plt.title(f'{dataset_name} ({method_name}) Feature Loadings')\n",
    "            filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_heatmap_L1.png\")\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        if method_name == \"NMF\":\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            # Use the NMF components matrix for the heatmap instead of the transformed data\n",
    "            sns.heatmap(nmf_components_matrix[:5, :], cmap='coolwarm')\n",
    "            plt.title(f'{dataset_name} ({method_name}) Feature Loadings')\n",
    "            filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_heatmap_L1.png\")\n",
    "            plt.savefig(filename, bbox_inches='tight')\n",
    "            plt.show()\n",
    "        \n",
    "        # --------------------- 2D Pairwise Scatter Plots ---------------------\n",
    "        n_components = min(result.shape[1], 5)\n",
    "        pair_df = pd.DataFrame(result[:, :n_components], columns=[f'{method_name} {i+1}' for i in range(n_components)])\n",
    "        if group_labels is not None:\n",
    "            pair_df['group'] = group_labels.values\n",
    "            pairplot = sns.pairplot(pair_df, hue='group', diag_kind=\"kde\", palette=custom_colors)\n",
    "        else:\n",
    "            pairplot = sns.pairplot(pair_df, diag_kind=\"kde\")\n",
    "        plt.suptitle(f'Pairwise 2D Components for {dataset_name} ({method_name})', y=1.02)\n",
    "        filename = os.path.join(output_dir, f\"{dataset_name}_{method_name}_pairplot_L1.png\")\n",
    "        pairplot.fig.savefig(filename, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    apply_dim_reduction(dataset_name, df)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Dimensionality Reduction Summary ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Save the summary results to a CSV file (with _L1 in the filename)\n",
    "results_df.to_csv(\"dimensionality_reduction_summary_L1.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
