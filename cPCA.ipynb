{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b6e8c-dde9-4508-933b-3a57ebcea37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'\n",
    "#os.environ['OMP_NUM_THREADS'] = '1'\n",
    "from sklearn.decomposition import PCA, FastICA, NMF\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from contrastive import CPCA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.linalg import eigh\n",
    "from numpy.linalg import pinv, norm\n",
    "from scipy.stats import kurtosis\n",
    "from itertools import combinations\n",
    "import time\n",
    "from sklearn.impute import SimpleImputer\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import kaleido\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcf1aef-7946-4787-ad50-4d23d61dc189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cPCA with Frobenius error\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create output directory for plots if it doesn't exist\n",
    "output_dir = \"output_plots\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Assemble datasets into a dictionary.\n",
    "# Variables autismcombinedcommon, twitch, accent, dim512, and dim1024 should be defined beforehand.\n",
    "datasets = {\n",
    "    \"Combined Autism\": autismcombinedcommon,  # expected dummy columns: Group_Adult, Group_Child, hidden: Group_Adolescent\n",
    "    \"Twitch\": twitch,\n",
    "    \"Accent\": accent,                          # expected dummy columns: language_US, language_FR, language_IT, language_GE, language_UK, hidden: language_ES\n",
    "    \"Dim512\": dim512,\n",
    "    \"Dim1024\": dim1024\n",
    "}\n",
    "\n",
    "####################################\n",
    "# Helper functions for group labeling\n",
    "####################################\n",
    "def get_autism_group(row):\n",
    "    if row.get(\"Group_Adult\", 0) == 1:\n",
    "        return \"Group_Adult\"\n",
    "    elif row.get(\"Group_Child\", 0) == 1:\n",
    "        return \"Group_Child\"\n",
    "    else:\n",
    "        return \"Group_Adolescent\"\n",
    "\n",
    "def get_accent_group(row):\n",
    "    if row.get(\"language_US\", 0) == 1:\n",
    "        return \"language_US\"\n",
    "    elif row.get(\"language_FR\", 0) == 1:\n",
    "        return \"language_FR\"\n",
    "    elif row.get(\"language_IT\", 0) == 1:\n",
    "        return \"language_IT\"\n",
    "    elif row.get(\"language_GE\", 0) == 1:\n",
    "        return \"language_GE\"\n",
    "    elif row.get(\"language_UK\", 0) == 1:\n",
    "        return \"language_UK\"\n",
    "    else:\n",
    "        return \"language_ES\"\n",
    "\n",
    "####################################\n",
    "# Custom regularized pseudo-inverse using SVD (with adjustable tolerance)\n",
    "####################################\n",
    "def regularized_pinv(A, tol=1e-1):\n",
    "    U, s, Vh = np.linalg.svd(A, full_matrices=False)\n",
    "    s_inv = np.array([1/x if x > tol else 0 for x in s])\n",
    "    return (Vh.T * s_inv) @ U.T\n",
    "\n",
    "####################################\n",
    "# Function to tune over candidate n_components with fixed alpha.\n",
    "####################################\n",
    "def tune_cpca_n_components(X_target, X_background, fixed_alpha=2.0, max_components=None):\n",
    "    if max_components is None:\n",
    "        max_components = np.linalg.matrix_rank(np.cov(X_target, rowvar=False))\n",
    "    best_n = 1\n",
    "    best_error = np.inf\n",
    "    best_eigvecs = None\n",
    "    best_eigvals = None\n",
    "    fro_errors = []\n",
    "    n_components_list = []\n",
    "    \n",
    "    # Compute contrastive covariance matrices\n",
    "    cov_target = np.cov(X_target, rowvar=False)\n",
    "    cov_background = np.cov(X_background, rowvar=False)\n",
    "    contrastive_cov = cov_target - fixed_alpha * cov_background\n",
    "    \n",
    "    # Eigen-decomposition (using eigh since covariance matrices are symmetric)\n",
    "    eigvals, eigvecs = eigh(contrastive_cov)\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    \n",
    "    for n in range(1, max_components + 1):\n",
    "        X_proj = X_target @ eigvecs[:, :n]\n",
    "        try:\n",
    "            # Reconstruct target from projection\n",
    "            X_reconstructed = X_proj @ pinv(eigvecs[:, :n], rcond=1e-5)\n",
    "        except np.linalg.LinAlgError:\n",
    "            fro_errors.append(np.inf)\n",
    "            n_components_list.append(n)\n",
    "            continue\n",
    "        error = norm(X_target - X_reconstructed, 'fro')\n",
    "        fro_errors.append(error)\n",
    "        n_components_list.append(n)\n",
    "        if error < best_error:\n",
    "            best_n = n\n",
    "            best_error = error\n",
    "            best_eigvecs = eigvecs[:, :n]\n",
    "            best_eigvals = eigvals[:n]\n",
    "    return best_n, best_eigvecs, best_eigvals, best_error, fro_errors, n_components_list\n",
    "\n",
    "####################################\n",
    "# Function to select target and background splits.\n",
    "####################################\n",
    "def select_cpca_split(df, dataset_name):\n",
    "    if dataset_name == \"Combined Autism\":\n",
    "        target = df[df['Group_Adult'] == 1]\n",
    "        background = df[df['Group_Adult'] != 1]\n",
    "    elif dataset_name == \"Accent\":\n",
    "        target = df[df['language_US'] == 1]\n",
    "        background = df[df['language_US'] != 1]\n",
    "    else:\n",
    "        split_size = 0.3\n",
    "        target = df.sample(frac=split_size, random_state=42)\n",
    "        background = df.drop(target.index)\n",
    "    return target, background\n",
    "\n",
    "####################################\n",
    "# Visualization Functions\n",
    "####################################\n",
    "def visualize_cpca(dataset_name, cpca_result, cpca_eigvals, error_type='Frobenius', group_labels=None, custom_colors=None):\n",
    "    n_components = cpca_result.shape[1]\n",
    "    # 3D Scatter Plot\n",
    "    if n_components >= 3:\n",
    "        df_proj = pd.DataFrame(cpca_result[:, :3], columns=[f'cPCA {i+1}' for i in range(3)])\n",
    "        if group_labels is not None:\n",
    "            df_proj['group'] = group_labels.to_numpy()\n",
    "            fig = px.scatter_3d(df_proj, x='cPCA 1', y='cPCA 2', z='cPCA 3',\n",
    "                                color='group',\n",
    "                                title=f'3D Scatter Plot for {dataset_name} (cPCA {error_type})',\n",
    "                                color_discrete_sequence=custom_colors)\n",
    "        else:\n",
    "            fig = px.scatter_3d(df_proj, x='cPCA 1', y='cPCA 2', z='cPCA 3',\n",
    "                                title=f'3D Scatter Plot for {dataset_name} (cPCA {error_type})')\n",
    "        fig.write_image(os.path.join(output_dir, f\"{dataset_name}_cPCA_3D_{error_type}.png\"))\n",
    "        fig.show()\n",
    "    elif n_components == 2:\n",
    "        df_proj = pd.DataFrame(cpca_result[:, :2], columns=[f'cPCA {i+1}' for i in range(2)])\n",
    "        if group_labels is not None:\n",
    "            df_proj['group'] = group_labels.to_numpy()\n",
    "            fig = px.scatter(df_proj, x='cPCA 1', y='cPCA 2',\n",
    "                           title=f'2D Scatter Plot for {dataset_name} (cPCA {error_type})',\n",
    "                           color='group', color_discrete_sequence=custom_colors)\n",
    "        else:\n",
    "            fig = px.scatter(df_proj, x='cPCA 1', y='cPCA 2',\n",
    "                           title=f'2D Scatter Plot for {dataset_name} (cPCA {error_type})')\n",
    "        fig.write_image(os.path.join(output_dir, f\"{dataset_name}_cPCA_2D_{error_type}.png\"))\n",
    "        fig.show()\n",
    "    elif n_components == 1:\n",
    "        df_proj = pd.DataFrame(cpca_result[:, :1], columns=['cPCA 1'])\n",
    "        fig = px.histogram(df_proj, x='cPCA 1',\n",
    "                           title=f'Histogram for {dataset_name} (cPCA - 1 Component {error_type})')\n",
    "        fig.write_image(os.path.join(output_dir, f\"{dataset_name}_cPCA_histogram_{error_type}.png\"))\n",
    "        fig.show()\n",
    "    \n",
    "    # Scree Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(cpca_eigvals) + 1), cpca_eigvals, marker='o')\n",
    "    plt.xlabel(\"Component\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(f\"Scree Plot for {dataset_name} (cPCA {error_type})\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f\"{dataset_name}_cPCA_scree_{error_type}.png\"), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2D Pairwise Scatter Plots with Coloring\n",
    "    if n_components >= 2:\n",
    "        n = min(n_components, 5)\n",
    "        pair_df = pd.DataFrame(cpca_result[:, :n], columns=[f'cPCA{i+1}' for i in range(n)])\n",
    "        if group_labels is not None:\n",
    "            pair_df['group'] = group_labels.to_numpy()\n",
    "            pairplot = sns.pairplot(pair_df, hue='group', diag_kind=\"kde\", palette=custom_colors)\n",
    "        else:\n",
    "            pairplot = sns.pairplot(pair_df, diag_kind=\"kde\")\n",
    "        plt.suptitle(f\"{dataset_name}_cPCA_pairplot_{error_type}\", y=1.02)\n",
    "        pairplot.fig.savefig(os.path.join(output_dir, f\"{dataset_name}_cPCA_pairplot_{error_type}.png\"), bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def visualize_cpca_errors(dataset_name, fro_errors_dict, n_components_list):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for alpha, error in fro_errors_dict.items():\n",
    "        plt.plot(n_components_list, [error] * len(n_components_list), label=f'Frobenius α={alpha}', linestyle='--')\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Frobenius Reconstruction Error\")\n",
    "    plt.title(f\"cPCA Errors vs Components for {dataset_name} (Frobenius)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f\"{dataset_name}_cPCA_errors_Frobenius.png\"), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "####################################\n",
    "# New: Visualization Function for cPCA Heatmap of Feature Loadings\n",
    "####################################\n",
    "def visualize_cpca_heatmap(dataset_name, cpca_loadings, feature_names, error_type='Frobenius'):\n",
    "    \"\"\"\n",
    "    Visualizes a heatmap of cPCA loadings.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        cpca_loadings (np.ndarray): The eigenvectors (loadings) computed by cPCA.\n",
    "                                    Expected shape is (n_features, n_components).\n",
    "        feature_names (list or pd.Index): Names of the features (after dropping dummy columns).\n",
    "        error_type (str): A label to indicate which error type was used (e.g., 'Frobenius').\n",
    "    \"\"\"\n",
    "    # Transpose so that rows are components and columns are features.\n",
    "    loadings_matrix = cpca_loadings.T  # now shape is (n_components, n_features)\n",
    "    n_components = loadings_matrix.shape[0]\n",
    "    n_to_plot = min(n_components, 5)  # Plot only the first 5 components for clarity\n",
    "\n",
    "    plt.figure(figsize=(max(10, len(feature_names) * 0.4), 6))\n",
    "    sns.heatmap(\n",
    "        loadings_matrix[:n_to_plot, :],\n",
    "        cmap='coolwarm',\n",
    "        xticklabels=feature_names,\n",
    "        yticklabels=[f'cPCA {i+1}' for i in range(n_to_plot)]\n",
    "    )\n",
    "    plt.title(f'{dataset_name} (cPCA {error_type}) Feature Loadings')\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Component\")\n",
    "    \n",
    "    filename = os.path.join(output_dir, f\"{dataset_name}_cPCA_heatmap_{error_type}.png\")\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "####################################\n",
    "# MAIN cPCA REDUCTION FUNCTION\n",
    "####################################\n",
    "def apply_cpca_reduction(dataset_name, df):\n",
    "    print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "    # Exclude dummy columns from cPCA computation.\n",
    "    if dataset_name == \"Accent\":\n",
    "        dummy_cols = [col for col in df.columns if col.startswith(\"language_\")]\n",
    "    else:\n",
    "        dummy_cols = [col for col in df.columns if col.startswith(\"Group_\")]\n",
    "    X_full = df.drop(columns=dummy_cols).values\n",
    "\n",
    "    # Branch based on dataset for target and background splitting.\n",
    "    if dataset_name == \"Combined Autism\":\n",
    "        target_df = df[df['Group_Adult'] == 1].copy()\n",
    "        X_target = target_df.drop(columns=dummy_cols).values\n",
    "        X_background = df[df['Group_Adult'] != 1].drop(columns=dummy_cols).values\n",
    "\n",
    "        # Tune cPCA normally first:\n",
    "        fixed_alpha = 2.6\n",
    "        max_components = np.linalg.matrix_rank(np.cov(X_target, rowvar=False))\n",
    "        best_n, best_eigvecs, best_eigvals, _, _, _ = tune_cpca_n_components(\n",
    "            X_target, X_background, fixed_alpha=fixed_alpha, max_components=max_components)\n",
    "    \n",
    "        # Project target onto cPCA space to identify outliers.\n",
    "        X_target_proj = X_target @ best_eigvecs\n",
    "    \n",
    "        # For example, identify outliers based on the 4th cPCA component (adjust threshold visually):\n",
    "        non_outlier_indices = (X_target_proj[:, 3] > -0.5)\n",
    "    \n",
    "        print(f\"Original adult count: {X_target.shape[0]}\")\n",
    "        X_target_filtered = X_target[non_outlier_indices]\n",
    "        print(f\"Filtered adult count: {X_target_filtered.shape[0]}\")\n",
    "    \n",
    "        # Re-run cPCA tuning with filtered target.\n",
    "        best_n, best_eigvecs, best_eigvals, cpca_fro_error, fro_errors_list, n_components_list = tune_cpca_n_components(\n",
    "            X_target_filtered, X_background, fixed_alpha=fixed_alpha, max_components=max_components)\n",
    "    \n",
    "        print(f\"  ✅ Filtered cPCA (target) selected {best_n} components with alpha = {fixed_alpha} and target error = {cpca_fro_error:.4f}\")\n",
    "        \n",
    "        # Project the full dataset using the learned eigenvectors.\n",
    "        cpca_result_full = X_full @ best_eigvecs\n",
    "        X_reconstructed_full = cpca_result_full @ pinv(best_eigvecs, rcond=1e-5)\n",
    "        full_error = norm(X_full - X_reconstructed_full, 'fro')\n",
    "        # Compute full relative error based on X_full.\n",
    "        full_relative_error = full_error / norm(X_full, 'fro')\n",
    "        print(f\"  Full dataset reconstruction error: {full_error:.4f}\")\n",
    "        \n",
    "    elif dataset_name == \"Accent\":\n",
    "        target_df = df[df['language_US'] == 1].copy()\n",
    "        X_target = target_df.drop(columns=dummy_cols).values\n",
    "        X_background = df[df['language_US'] != 1].drop(columns=dummy_cols).values\n",
    "    \n",
    "        # Tune cPCA normally first:\n",
    "        fixed_alpha = 2.6\n",
    "        max_components = np.linalg.matrix_rank(np.cov(X_target, rowvar=False))\n",
    "        best_n, best_eigvecs, best_eigvals, _, _, _ = tune_cpca_n_components(\n",
    "            X_target, X_background, fixed_alpha=fixed_alpha, max_components=max_components)\n",
    "    \n",
    "        # Project target onto cPCA space to identify outliers.\n",
    "        X_target_proj = X_target @ best_eigvecs\n",
    "    \n",
    "        # For example, identify outliers based on the first and third cPCA components:\n",
    "        non_outlier_indices = (X_target_proj[:, 0] > -0.1) & (X_target_proj[:, 2] > 0.3)\n",
    "    \n",
    "        print(f\"Original US speaker count: {X_target.shape[0]}\")\n",
    "        X_target_filtered = X_target[non_outlier_indices]\n",
    "        print(f\"Filtered US speaker count: {X_target_filtered.shape[0]}\")\n",
    "    \n",
    "        # Re-run cPCA tuning with filtered target.\n",
    "        best_n, best_eigvecs, best_eigvals, cpca_fro_error, fro_errors_list, n_components_list = tune_cpca_n_components(\n",
    "            X_target_filtered, X_background, fixed_alpha=fixed_alpha, max_components=max_components)\n",
    "    \n",
    "        print(f\"  ✅ Filtered cPCA (target) selected {best_n} components with alpha = {fixed_alpha} and target error = {cpca_fro_error:.4f}\")\n",
    "        \n",
    "        # Project the full dataset using the learned eigenvectors.\n",
    "        cpca_result_full = X_full @ best_eigvecs\n",
    "        X_reconstructed_full = cpca_result_full @ pinv(best_eigvecs, rcond=1e-5)\n",
    "        full_error = norm(X_full - X_reconstructed_full, 'fro')\n",
    "        full_relative_error = full_error / norm(X_full, 'fro')\n",
    "        print(f\"  Full dataset reconstruction error: {full_error:.4f}\")\n",
    "        \n",
    "    else:\n",
    "        target_df, background_df = select_cpca_split(df, dataset_name)\n",
    "        X_target = target_df.drop(columns=dummy_cols).values\n",
    "        X_background = background_df.drop(columns=dummy_cols).values\n",
    "        \n",
    "        fixed_alpha = 2.6\n",
    "        max_components = np.linalg.matrix_rank(np.cov(X_target, rowvar=False))\n",
    "        best_n, best_eigvecs, best_eigvals, cpca_fro_error, fro_errors_list, n_components_list = tune_cpca_n_components(\n",
    "            X_target, X_background, fixed_alpha=fixed_alpha, max_components=max_components)\n",
    "        print(f\"  ✅ cPCA (target) selected {best_n} components with alpha = {fixed_alpha} and target error = {cpca_fro_error:.4f}\")\n",
    "    \n",
    "        cpca_result_full = X_full @ best_eigvecs\n",
    "        X_reconstructed_full = cpca_result_full @ pinv(best_eigvecs, rcond=1e-5)\n",
    "        full_error = norm(X_full - X_reconstructed_full, 'fro')\n",
    "        full_relative_error = full_error / norm(X_full, 'fro')\n",
    "        print(f\"  Full dataset reconstruction error: {full_error:.4f}\")\n",
    "    \n",
    "    # Derive group labels for visualization.\n",
    "    if dataset_name == \"Accent\":\n",
    "        expected = [\"language_US\", \"language_FR\", \"language_ES\", \"language_IT\", \"language_GE\", \"language_UK\"]\n",
    "        group_labels = df.apply(get_accent_group, axis=1)\n",
    "        group_labels = pd.Categorical(group_labels, categories=expected)\n",
    "        custom_colors = px.colors.qualitative.Plotly[:len(expected)]\n",
    "    elif dataset_name == \"Combined Autism\":\n",
    "        group_labels = df.apply(get_autism_group, axis=1)\n",
    "        group_labels = pd.Categorical(group_labels, categories=[\"Group_Adult\", \"Group_Child\", \"Group_Adolescent\"])\n",
    "        custom_colors = px.colors.qualitative.Plotly[:3]\n",
    "    else:\n",
    "        group_labels, custom_colors = None, None\n",
    "\n",
    "    # Visualize the full dataset projection.\n",
    "    visualize_cpca(dataset_name, cpca_result_full, best_eigvals, error_type='Frobenius', group_labels=group_labels, custom_colors=custom_colors)\n",
    "    \n",
    "    # ---- New: Visualize the cPCA loadings as a heatmap ----\n",
    "    feature_names = df.drop(columns=dummy_cols).columns\n",
    "    visualize_cpca_heatmap(dataset_name, best_eigvecs, feature_names, error_type='Frobenius')\n",
    "    \n",
    "    # Append results for later summary.\n",
    "    comparison_results.append({\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"cPCA Best Alpha (Frobenius)\": round(fixed_alpha, 4),\n",
    "        \"cPCA Components (Frobenius)\": best_n,\n",
    "        \"cPCA Frobenius Error (target)\": round(cpca_fro_error, 4),\n",
    "        \"cPCA Full Error\": round(full_error, 4),\n",
    "        \"cPCA Full Relative Error\": round(full_relative_error, 4)\n",
    "    })\n",
    "    print(f\"✅ Completed {dataset_name}\")\n",
    "\n",
    "####################################\n",
    "# Process all datasets.\n",
    "####################################\n",
    "comparison_results = []\n",
    "for dataset_name, df in datasets.items():\n",
    "    apply_cpca_reduction(dataset_name, df)\n",
    "\n",
    "# Final summary table.\n",
    "results_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\n--- Final cPCA Summary ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Save summary results to CSV.\n",
    "results_df.to_csv(\"cpca_results_summary.csv\", index=False)\n",
    "print(\"\\nAll datasets processed and results saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fe7186-3782-4a3f-88c1-c4dda07fbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cPCA with L1 error\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Create output directory for plots if it doesn't exist\n",
    "output_dir = \"output_plots\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Assemble datasets into a dictionary.\n",
    "# Variables autismcombinedcommon, twitch, accent, dim512, and dim1024 should be defined beforehand.\n",
    "datasets = {\n",
    "    \"Combined Autism\": autismcombinedcommon,  # expected dummy columns: Group_Adult, Group_Child, hidden: Group_Adolescent\n",
    "    \"Twitch\": twitch,\n",
    "    \"Accent\": accent,                          # expected dummy columns: language_US, language_FR, language_IT, language_GE, language_UK, hidden: language_ES\n",
    "    \"Dim512\": dim512,\n",
    "    \"Dim1024\": dim1024\n",
    "}\n",
    "\n",
    "####################################\n",
    "# Helper functions for group labeling for visualization\n",
    "####################################\n",
    "def get_autism_group(row):\n",
    "    if row.get(\"Group_Adult\", 0) == 1:\n",
    "        return \"Group_Adult\"\n",
    "    elif row.get(\"Group_Child\", 0) == 1:\n",
    "        return \"Group_Child\"\n",
    "    else:\n",
    "        return \"Group_Adolescent\"\n",
    "\n",
    "def get_accent_group(row):\n",
    "    if row.get(\"language_US\", 0) == 1:\n",
    "        return \"language_US\"\n",
    "    elif row.get(\"language_FR\", 0) == 1:\n",
    "        return \"language_FR\"\n",
    "    elif row.get(\"language_IT\", 0) == 1:\n",
    "        return \"language_IT\"\n",
    "    elif row.get(\"language_GE\", 0) == 1:\n",
    "        return \"language_GE\"\n",
    "    elif row.get(\"language_UK\", 0) == 1:\n",
    "        return \"language_UK\"\n",
    "    else:\n",
    "        return \"language_ES\"\n",
    "\n",
    "####################################\n",
    "# Custom regularized pseudo-inverse using SVD (with adjustable tolerance)\n",
    "####################################\n",
    "def regularized_pinv(A, tol=1e-1):\n",
    "    U, s, Vh = np.linalg.svd(A, full_matrices=False)\n",
    "    s_inv = np.array([1/x if x > tol else 0 for x in s])\n",
    "    return (Vh.T * s_inv) @ U.T\n",
    "\n",
    "####################################\n",
    "# Function: tune over candidate n_components with fixed alpha using L1 error.\n",
    "####################################\n",
    "def tune_cpca_n_components(X_target, X_background, fixed_alpha=2.0, max_components=None):\n",
    "    if max_components is None:\n",
    "        max_components = np.linalg.matrix_rank(np.cov(X_target, rowvar=False))\n",
    "    best_n = 1\n",
    "    best_error = np.inf\n",
    "    best_eigvecs = None\n",
    "    best_eigvals = None\n",
    "    L1_errors = []\n",
    "    n_components_list = []\n",
    "    \n",
    "    # Compute contrastive covariance matrices\n",
    "    cov_target = np.cov(X_target, rowvar=False)\n",
    "    cov_background = np.cov(X_background, rowvar=False)\n",
    "    contrastive_cov = cov_target - fixed_alpha * cov_background\n",
    "    \n",
    "    # Eigen-decomposition (using eigh since covariance is symmetric)\n",
    "    eigvals, eigvecs = eigh(contrastive_cov)\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    \n",
    "    for n in range(1, max_components + 1):\n",
    "        X_proj = X_target @ eigvecs[:, :n]\n",
    "        try:\n",
    "            # Reconstruct target from projection\n",
    "            X_reconstructed = X_proj @ pinv(eigvecs[:, :n], rcond=1e-5)\n",
    "        except np.linalg.LinAlgError:\n",
    "            L1_errors.append(np.inf)\n",
    "            n_components_list.append(n)\n",
    "            continue\n",
    "        error = np.sum(np.abs(X_target - X_reconstructed))\n",
    "        L1_errors.append(error)\n",
    "        n_components_list.append(n)\n",
    "        if error < best_error:\n",
    "            best_n = n\n",
    "            best_error = error\n",
    "            best_eigvecs = eigvecs[:, :n]\n",
    "            best_eigvals = eigvals[:n]\n",
    "    return best_n, best_eigvecs, best_eigvals, best_error, L1_errors, n_components_list\n",
    "\n",
    "####################################\n",
    "# Function to select target and background splits.\n",
    "####################################\n",
    "def select_cpca_split(df, dataset_name):\n",
    "    if dataset_name == \"Combined Autism\":\n",
    "        target = df[df['Group_Adult'] == 1]\n",
    "        background = df[df['Group_Adult'] != 1]\n",
    "    elif dataset_name == \"Accent\":\n",
    "        target = df[df['language_US'] == 1]\n",
    "        background = df[df['language_US'] != 1]\n",
    "    else:\n",
    "        split_size = 0.3\n",
    "        target = df.sample(frac=split_size, random_state=42)\n",
    "        background = df.drop(target.index)\n",
    "    return target, background\n",
    "\n",
    "####################################\n",
    "# Visualization Functions\n",
    "####################################\n",
    "def visualize_cpca(dataset_name, cpca_result, cpca_eigvals, error_type='L1', group_labels=None, custom_colors=None):\n",
    "    n_components = cpca_result.shape[1]\n",
    "    # 3D Scatter Plot\n",
    "    if n_components >= 3:\n",
    "        df_proj = pd.DataFrame(cpca_result[:, :3], columns=[f'cPCA {i+1}' for i in range(3)])\n",
    "        if group_labels is not None:\n",
    "            df_proj['group'] = group_labels.to_numpy()\n",
    "            fig = px.scatter_3d(df_proj, x='cPCA 1', y='cPCA 2', z='cPCA 3',\n",
    "                                color='group',\n",
    "                                title=f'3D Scatter Plot for {dataset_name} (cPCA {error_type})',\n",
    "                                color_discrete_sequence=custom_colors)\n",
    "        else:\n",
    "            fig = px.scatter_3d(df_proj, x='cPCA 1', y='cPCA 2', z='cPCA 3',\n",
    "                                title=f'3D Scatter Plot for {dataset_name} (cPCA {error_type})')\n",
    "        fig.write_image(os.path.join(output_dir, f\"{dataset_name}_cPCA_3D_{error_type}.png\"))\n",
    "        fig.show()\n",
    "    elif n_components == 2:\n",
    "        df_proj = pd.DataFrame(cpca_result[:, :2], columns=[f'cPCA {i+1}' for i in range(2)])\n",
    "        if group_labels is not None:\n",
    "            df_proj['group'] = group_labels.to_numpy()\n",
    "            fig = px.scatter(df_proj, x='cPCA 1', y='cPCA 2',\n",
    "                           title=f'2D Scatter Plot for {dataset_name} (cPCA {error_type})',\n",
    "                           color='group', color_discrete_sequence=custom_colors)\n",
    "        else:\n",
    "            fig = px.scatter(df_proj, x='cPCA 1', y='cPCA 2',\n",
    "                           title=f'2D Scatter Plot for {dataset_name} (cPCA {error_type})')\n",
    "        fig.write_image(os.path.join(output_dir, f\"{dataset_name}_cPCA_2D_{error_type}.png\"))\n",
    "        fig.show()\n",
    "    elif n_components == 1:\n",
    "        df_proj = pd.DataFrame(cpca_result[:, :1], columns=['cPCA 1'])\n",
    "        fig = px.histogram(df_proj, x='cPCA 1',\n",
    "                           title=f'Histogram for {dataset_name} (cPCA - 1 Component {error_type})')\n",
    "        fig.write_image(os.path.join(output_dir, f\"{dataset_name}_cPCA_histogram_{error_type}.png\"))\n",
    "        fig.show()\n",
    "    \n",
    "    # Scree Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(cpca_eigvals) + 1), cpca_eigvals, marker='o')\n",
    "    plt.xlabel(\"Component\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(f\"Scree Plot for {dataset_name} (cPCA {error_type})\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f\"{dataset_name}_cPCA_scree_{error_type}.png\"), bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # 2D Pairwise Scatter Plots with Coloring\n",
    "    if n_components >= 2:\n",
    "        n = min(n_components, 5)\n",
    "        pair_df = pd.DataFrame(cpca_result[:, :n], columns=[f'cPCA{i+1}' for i in range(n)])\n",
    "        if group_labels is not None:\n",
    "            pair_df['group'] = group_labels.to_numpy()\n",
    "            pairplot = sns.pairplot(pair_df, hue='group', diag_kind=\"kde\", palette=custom_colors)\n",
    "        else:\n",
    "            pairplot = sns.pairplot(pair_df, diag_kind=\"kde\")\n",
    "        plt.suptitle(f\"{dataset_name}_cPCA_pairplot_{error_type}\", y=1.02)\n",
    "        pairplot.fig.savefig(os.path.join(output_dir, f\"{dataset_name}_cPCA_pairplot_{error_type}.png\"), bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "def visualize_cpca_errors(dataset_name, L1_errors_dict, n_components_list):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for alpha, error in L1_errors_dict.items():\n",
    "        plt.plot(n_components_list, [error] * len(n_components_list), label=f'L1 α={alpha}', linestyle='--')\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"L1 Reconstruction Error\")\n",
    "    plt.title(f\"cPCA Errors vs Components for {dataset_name} (L1)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(output_dir, f\"{dataset_name}_cPCA_errors_L1.png\"), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "####################################\n",
    "# New: Visualization Function for cPCA Heatmap of Feature Loadings\n",
    "####################################\n",
    "def visualize_cpca_heatmap(dataset_name, cpca_loadings, feature_names, error_type='L1'):\n",
    "    \"\"\"\n",
    "    Visualizes a heatmap of cPCA loadings.\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        cpca_loadings (np.ndarray): The eigenvectors (loadings) computed by cPCA.\n",
    "                                    Expected shape is (n_features, n_components).\n",
    "        feature_names (list or pd.Index): Names of the features (after dropping dummy columns).\n",
    "        error_type (str): A label to indicate which error type was used (e.g., 'L1').\n",
    "    \"\"\"\n",
    "    # Transpose so that rows are components and columns are features.\n",
    "    loadings_matrix = cpca_loadings.T  # now shape is (n_components, n_features)\n",
    "    n_components = loadings_matrix.shape[0]\n",
    "    n_to_plot = min(n_components, 5)  # Plot only the first 5 components for clarity\n",
    "\n",
    "    plt.figure(figsize=(max(10, len(feature_names) * 0.4), 6))\n",
    "    sns.heatmap(\n",
    "        loadings_matrix[:n_to_plot, :],\n",
    "        cmap='coolwarm',\n",
    "        xticklabels=feature_names,\n",
    "        yticklabels=[f'cPCA {i+1}' for i in range(n_to_plot)]\n",
    "    )\n",
    "    plt.title(f'{dataset_name} (cPCA {error_type}) Feature Loadings')\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Component\")\n",
    "    \n",
    "    filename = os.path.join(output_dir, f\"{dataset_name}_cPCA_heatmap_{error_type}.png\")\n",
    "    plt.savefig(filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "####################################\n",
    "# MAIN cPCA REDUCTION FUNCTION (using L1 error)\n",
    "####################################\n",
    "def apply_cpca_reduction(dataset_name, df):\n",
    "    print(f\"\\n--- Processing {dataset_name} ---\")\n",
    "    # Exclude dummy columns from cPCA computation.\n",
    "    if dataset_name == \"Accent\":\n",
    "        dummy_cols = [col for col in df.columns if col.startswith(\"language_\")]\n",
    "    else:\n",
    "        dummy_cols = [col for col in df.columns if col.startswith(\"Group_\")]\n",
    "    X_full = df.drop(columns=dummy_cols).values\n",
    "\n",
    "    # Define target and background splits.\n",
    "    if dataset_name == \"Combined Autism\":\n",
    "        target_df = df[df['Group_Adult'] == 1].copy()\n",
    "        X_target = target_df.drop(columns=dummy_cols).values\n",
    "        X_background = df[df['Group_Adult'] != 1].drop(columns=dummy_cols).values\n",
    "    elif dataset_name == \"Accent\":\n",
    "        target_df = df[df['language_US'] == 1].copy()\n",
    "        X_target = target_df.drop(columns=dummy_cols).values\n",
    "        X_background = df[df['language_US'] != 1].drop(columns=dummy_cols).values\n",
    "    else:\n",
    "        target_df, background_df = select_cpca_split(df, dataset_name)\n",
    "        X_target = target_df.drop(columns=dummy_cols).values\n",
    "        X_background = background_df.drop(columns=dummy_cols).values\n",
    "\n",
    "    # Tune cPCA over candidate n_components only, fixing alpha.\n",
    "    fixed_alpha = 2.6\n",
    "    max_components = np.linalg.matrix_rank(np.cov(X_target, rowvar=False))\n",
    "    best_n, best_eigvecs, best_eigvals, cpca_L1_error, L1_errors_list, n_components_list = tune_cpca_n_components(\n",
    "        X_target, X_background, fixed_alpha=fixed_alpha, max_components=max_components)\n",
    "    print(f\"  ✅ cPCA (target) selected {best_n} components with alpha = {fixed_alpha} and target error = {cpca_L1_error:.4f}\")\n",
    "    \n",
    "    # Project the full dataset using the learned eigenvectors.\n",
    "    cpca_result_full = X_full @ best_eigvecs\n",
    "    X_reconstructed_full = cpca_result_full @ pinv(best_eigvecs, rcond=1e-5)\n",
    "    full_error = mean_absolute_error(X_full, X_reconstructed_full)\n",
    "    # Compute the full relative error for L1 as:\n",
    "    full_relative_error = full_error / np.sum(np.abs(X_full))\n",
    "    print(f\"  Full dataset reconstruction error: {full_error:.4f}\")\n",
    "    print(f\"  Full dataset relative reconstruction error: {full_relative_error:.4f}\")\n",
    "    \n",
    "    # For visualization: derive group labels.\n",
    "    if dataset_name == \"Accent\":\n",
    "        expected = [\"language_US\", \"language_FR\", \"language_ES\", \"language_IT\", \"language_GE\", \"language_UK\"]\n",
    "        group_labels = df.apply(get_accent_group, axis=1)\n",
    "        group_labels = pd.Categorical(group_labels, categories=expected)\n",
    "        custom_colors = px.colors.qualitative.Plotly[:len(expected)]\n",
    "    elif dataset_name == \"Combined Autism\":\n",
    "        group_labels = df.apply(get_autism_group, axis=1)\n",
    "        group_labels = pd.Categorical(group_labels, categories=[\"Group_Adult\", \"Group_Child\", \"Group_Adolescent\"])\n",
    "        custom_colors = px.colors.qualitative.Plotly[:3]\n",
    "    else:\n",
    "        group_labels, custom_colors = None, None\n",
    "\n",
    "    # Visualize the full dataset projection.\n",
    "    visualize_cpca(dataset_name, cpca_result_full, best_eigvals, error_type='L1', group_labels=group_labels, custom_colors=custom_colors)\n",
    "    \n",
    "    # ---- New: Visualize the cPCA loadings as a heatmap ----\n",
    "    feature_names = df.drop(columns=dummy_cols).columns\n",
    "    visualize_cpca_heatmap(dataset_name, best_eigvecs, feature_names, error_type='L1')\n",
    "    \n",
    "    # Append results for later summary.\n",
    "    comparison_results.append({\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"cPCA Best Alpha (L1)\": round(fixed_alpha, 4),\n",
    "        \"cPCA Components (L1)\": best_n,\n",
    "        \"cPCA L1 Error (target)\": round(cpca_L1_error, 4),\n",
    "        \"cPCA Full Error\": round(full_error, 4),\n",
    "        \"cPCA Full Relative Error\": round(full_relative_error, 4)\n",
    "    })\n",
    "    print(f\"✅ Completed {dataset_name}\")\n",
    "\n",
    "####################################\n",
    "# Process all datasets.\n",
    "####################################\n",
    "comparison_results = []\n",
    "for dataset_name, df in datasets.items():\n",
    "    apply_cpca_reduction(dataset_name, df)\n",
    "\n",
    "# Final summary table.\n",
    "results_df = pd.DataFrame(comparison_results)\n",
    "print(\"\\n--- Final cPCA Summary ---\")\n",
    "print(results_df)\n",
    "\n",
    "# Save summary results to CSV.\n",
    "results_df.to_csv(\"cpca_results_summary_L1.csv\", index=False)\n",
    "print(\"\\nAll datasets processed and results saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
